# RUNS
# * Best Params Gridsearch (x256):
#   | -m optimizer=adam,radam,amsgrad,rmsprop optimizer.lr=0.0033,0.001,0.00033,0.0001 framework=betavae,adavae dataset=xysquares,cars3d framework.beta=0.1,0.25,1.0,4.0
#   | >>> decent settings: beta=1, optimizer=radam, lr=5e-4
# * Flatness Metric (432)
#   | -m optimizer.lr=0.001,0.0005 framework.beta=0.25,1.0,4.0 framework=ae,betavae,adavae_os,adavae,tvae,X--adatvae dataset=cars3d,smallnorb,3dshapes,monte_rollouts,xyblocks,xyblocks_grey,xyobject,xyobject_grey,xysquares,xysquares_grey,xysquares_overlap,xysquares_overlap_grey


defaults:
  # experiment
  - framework: betavae
  - model: conv64
  - optimizer: radam
  - dataset: xysquares
  - augment: none
  - sampling: full_bb
  - metrics: all
  - schedule: beta_increase
  # runtime
  - run_length: short
  - run_location: cluster_many
  - run_callbacks: vis
  - run_logging: wandb
  # plugins
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  - hydra/launcher: submitit_slurm

job:
  user: 'n_michlo'
  project: 'DELETE-test-schedules'
  name: '${framework.name}:${framework.module.recon_loss}|${dataset.name}:${sampling.name}|${trainer.steps}'
  partition: batch

framework:
    beta: 0.001
    module:
      recon_loss: mse
      loss_reduction: mean

model:
  z_size: 9

optimizer:
  lr: 5e-4

# CUSTOM DEFAULTS SPECIALIZATION
# - This key is deleted on load and the correct key on the root config is set similar to defaults.
# - Unfortunately this hack needs to exists as hydra does not yet support this kinda of variable interpolation in defaults.
specializations:
  data_wrapper: ${dataset.data_type}_${framework.data_wrap_mode}
