
# ========================================================================= #
# CONFIG                                                                    #
# ========================================================================= #


defaults:
  - run_logging: wandb_fast
  - run_location: griffin
  - run_launcher: local
  # entries in this file override entries from default lists
  - _self_

settings:
  job:
    user: 'n_michlo'
    project: 'DELETE'  # exp-disentangle-dataset
    name: 'no-name' # TEST-${framework.dataset_name}_${framework.adversarial_mode}_${framework.sampler_name}_s${trainer.max_steps}_${framework.optimizer_name}_lr${framework.optimizer_lr} # _wd${framework.optimizer_kwargs.weight_decay}
    seed: 777
  exp:
    show_every_n_steps: 500
    # saving
    rel_save_dir: 'out/adversarial_data/'
    save_prefix: 'PREFIX'
    save_data: TRUE
  dataset:
    batch_size: 32

trainer:
  # same as defaults: - run_length: ...
  max_steps: 30001
  max_epochs: 30001

adv_system:
  ### IMPORTANT SETTINGS ###
  dataset_name: 'dsprites'          # [cars3d, smallnorb, dsprites, shapes3d, xysquares_8x8_mini]
  adversarial_mode: 'self'          # [self, invert_margin_0.005] invert, invert_unbounded
  sampler_name: 'close_p_random_n'  # [close_p_random_n, same_k1_close]

  ### OTHER SETTINGS ###
  # optimizer options
  optimizer_name: 'Adam'
  optimizer_lr: 1e-1
  optimizer_kwargs: NULL
  # dataset config options
  # | dataset_name: 'cars3d'  # cars3d, smallnorb, xysquares_8x8_mini
  dataset_batch_size: 2048  # x3
  dataset_num_workers: ${dataloader.num_workers}
  data_root: ${dsettings.storage.data_root}
  # adversarial loss options
  # | adversarial_mode: 'invert_margin_0.005'  # [self, invert_margin_0.005] invert, invert_unbounded
  adversarial_swapped: FALSE
  adversarial_masking: FALSE  # can produce weird artefacts that look like they might go against the training process, eg. improve disentanglement on dsprites, not actually checked by trianing model on this.
  adversarial_top_k: NULL  # NULL or range(1, batch_size)
  pixel_loss_mode: 'mse'
  # sampling config
  # | sampler_name: 'close_p_random_n'   # [close_p_random_n] (see notes above) -- close_p_random_n, close_p_random_n_bb, same_k, same_k_close, same_k1_close, same_k (might be wrong!), same_k_close, same_k1_close, close_far, close_factor_far_random, close_far_same_factor, same_factor, random_bb, random_swap_manhat, random_swap_manhat_norm
  # train options
  train_batch_optimizer: TRUE
  train_dataset_fp16: TRUE
