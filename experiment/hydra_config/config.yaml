defaults:
  - framework: adavae
  - model: conv64
  - optimizer: radam
  - dataset: xysquares
  # hydra plugins
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  - hydra/launcher: submitit_slurm

# ------------------ #
# EXPERIMENT OPTIONS #
# ------------------ #

# remember to set timeout limits below
trainer:
  epochs: 100000  # not resilient to change in dataset size, we rather limit by steps (len(dataset) / batch_size)
  steps: 100000
  cuda: TRUE
  prepare_data_per_node: TRUE

model:
  z_size: 9
  y_size: 9

dataset:
  num_workers: 8
  batch_size: 256
  data_dir: '/tmp/${env:USER}/datasets'
  try_in_memory: TRUE
  # varying factors (if applicable for pairs and triplets)
  k: 1
  n_k: 'uniform'

optimizer:
  lr: 1e-3

# --------------- #
# RUNTIME OPTIONS #
# --------------- #

logging:
  logs_dir: 'logs'
  # logging frequency in number of batchers, default: log_interval=50, save_log_interval=100
  log_interval: 100
  save_log_interval: 200
  # log to online service
  wandb:
    enabled: TRUE
    offline: FALSE
    name: 'uniform-${framework.name}-${model.z_size}-${dataset.name}-${optimizer.name}-${model.name}'
    project: 'test'
    group: null
    entity: '${env:USER}'
    tags:
      - '${framework.name}'
      - '${dataset.name}'
      - '${optimizer.name}'
      - '${model.name}'

callbacks:
  progress:
    interval: 30
  latent_cycle:
    seed: 7777
    every_n_steps: 1200
  metrics:
    every_n_steps: 3600

hydra:
  job:
    name: 'disent'
  launcher:
    params:
      partition: batch
      mem_gb: 0
      timeout_min: 1440  # minutes
      submitit_folder: '${hydra.sweep.dir}/submitit/%j'
  run:
    dir: '${logging.logs_dir}/hydra_run/${now:%Y-%m-%d_%H-%M-%S}_${hydra.job.name}'
  sweep:
    dir: '${logging.logs_dir}/hydra_sweep/${now:%Y-%m-%d_%H-%M-%S}_${hydra.job.name}'
    subdir: '${hydra.job.id}'  # hydra.job.id is not available for dir
